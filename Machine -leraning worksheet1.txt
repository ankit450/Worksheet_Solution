                                                                              WORKSHEET-1 ANSWER KEYS
                                                                               Machine Learning

1. C) between -1 and 1
2. D) Ridge Regularisation
3. C) hyperplane
4. A) Logistic Regression 
5. A) 2.205 × old coefficient of ‘X’ 
6. B) increases
7.C) Random Forests are easy to interpret


In Q8 to Q10, more than one options are correct, Choose all the correct options:

8. B) Principal Components are calculated using unsupervised learning techniques
   C) Principal Components are linear combinations of Linear Variables.

9.A) Identifying developed, developing and under-developed countries on the basis of factors like GDP, poverty
  index, employment rate, population and living index
  B) Identifying loan defaulters in a bank on the basis of previous years’ data of loan accounts.

10.A) max_depth B) max_features
   D) min_samples_leaf

Q11 to Q15 are subjective answer type questions, Answer them briefly.

11. What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection.

ans. An outlier is a piece of data that is an abnormal distance from other points. In other words, it’s data that lies outside the other values in the set. 

      Inter Quartile Range (IQR):

      IQR = Q3-Q1
      Where, Q1 = 25th %ile of the data
	Q2 = 50th %ile (a.k.a. median)
	Q3 = 75th %ile of the data.
      Upper bound = Q3 + 1.5*Q3
      Lower Bound = Q1 – 1.5*Q1

 12. What is the primary difference between bagging and boosting algorithms?
ans. agging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data. Boosting is an iterative technique which adjusts the weight of an observation based on the last classification.

Differences Between Bagging and Boosting –

         BAGGING	                                 BOOSTING

1.	Simplest way of combining predictions that       A way of combining predictions that
        belong to the same type.	                 belong to the different types.

2.	Aim to decrease variance, not bias.	         Aim to decrease bias, not variance.

3.	Each model receives equal weight.	         Models are weighted according to their performance.

4.	Each model is built independently.	         New models are influenced
                                                         by performance of previously built models.

5.	If the classifier is unstable (high variance),  If the classifier is stable and simple (high bias) the apply boosting.
         then apply bagging.
	                       
8.	Random forest.	                                 Gradient boosting.  


13. What is adjusted R2 in logistic regression. How is it calculated?        

ans. R-squared (R²) :-
      It measures the proportion of the variation in your dependent variable explained by all of your independent variables in the model. It assumes that every independent variable in the model helps to explain variation in the dependent variable. 
      In reality, some independent variables (predictors) don't help to explain dependent (target) variable. In other words, some variables do not contribute in predicting target variable.

    adjusted- R2 squared :- Adjusted R2 and R2 both represent that how well the model fits the data points. t measures the proportion of variation explained by only those independent variables that really help in explaining the dependent variable. It penalizes you for adding independent variable that do not help in predicting the dependen
                           Adjusted R-Squared can be calculated mathematically in terms of sum of squares. The only difference between R-square and Adjusted R-square equation is degree of freedom.

to calculate adjusted R2 is as follows:


R2adj = [(1-R2)(n-1)]/n-k-1]
 
Where, n = number of data points in the dataset
K = Number of features in the dataset excluding the constant term.

14. What is the difference between standardisation and normalisation?

ans. difference between standardisation and normalisation.
     The terms normalization and standardization are sometimes used interchangeably, but they usually refer to different things. 
     Normalization usually means to scale a variable to have a values between 0 and 1, 
     while standardization transforms data to have a mean of zero and a standard deviation of 1.

This standardization is called a z-score, and data points can be standardized with the following formula:

Normalization is often called min-max scaling. Formula for Normalization is as follows:
 
 
15. What is cross-validation? Describe one advantage and one disadvantage of using cross-validation.

Ans. Cross validation is a technique to fit a model on data set. In cross validation the data set is divided into ‘k’ number of sets 
where ‘k-1’ sets are used for training and 1 set is used as validation set.And this is done for all the set one by one and the final score of model is taken as average score of all the ‘k’ number of fits. 
Advantage of using Cross validation is that, there is no need of separate validation data, cross validation reduces chances of overfitting and gives a more generic model. 
Cross validation has a disadvantage that it takes more time to fit the model over a large dataset and the model built is more complex than the basic model.


